{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5f0ce6b-c4f0-46ae-88ef-20bbaa274eda",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "- [ ] Add example of dataset element\n",
    "- [X] Write documentation\n",
    "- [ ] Retrain the model\n",
    "- [ ] Add an evaluation function\n",
    "- [ ] Experiment with some values for the learning rate etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb467cc2",
   "metadata": {},
   "source": [
    "# Image Captioning using ViT and GPT-2\n",
    "The second part of the pipeline consists of taking the extracted images as input and transforming them to a human-understandable text. This problem is more commonly known as image captioning, and many pre-existing models exist. One of the most commonly used technique for image captioning is by using an image encoding to transform the image into an embedding which can then be used as input for a language model. This language model then decodes the embedding back into natural language. A commonly used combination is the ViT/GPT-2 pair.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5464d8a",
   "metadata": {},
   "source": [
    "### Sources\n",
    "The sources used in this notebook include:\n",
    "    - https://huggingface.co/nlpconnect/vit-gpt2-image-captioning\n",
    "    - https://huggingface.co/docs/transformers/model_doc/vit\n",
    "    - https://huggingface.co/docs/transformers/model_doc/gpt2\n",
    "    - https://github.com/NjtechCVLab/RSTPReid-Dataset\n",
    "    - https://vision.cornell.edu/se3/wp-content/uploads/2018/03/1501.pdf\n",
    "    - https://en.wikipedia.org/wiki/BLEU\n",
    "    - https://en.wikipedia.org/wiki/METEOR\n",
    "    - https://huggingface.co/spaces/evaluate-metric/rouge\n",
    "    \n",
    "    - The courses in the postgraduate \"AI and ML in business and engineering\" at KULeuven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0847fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env TF_ENABLE_ONEDNN_OPTS=0\n",
    "\n",
    "# Imports\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from torch.utils import tensorboard\n",
    "from transformers import GPT2TokenizerFast, ViTImageProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "from libs.data import RSTPReid\n",
    "from libs.engine import train_one_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "823da72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "MODEL_NAME = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "DATASET_PATH = Path().resolve().parent / 'RSTPReid'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b5f95d",
   "metadata": {},
   "source": [
    "## Load the model\n",
    "ViT (Vision Transformer) is a Transformer encoder, which was trained on ImageNet. It attained \"excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train\", according the the original paper by Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby.\n",
    "\n",
    "GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. It is trained with a simple objective: predict the next word, given all of the previous words within a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da545a31-7bc6-42b1-93eb-5fd1cc21cf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-22 17:42:25.059625: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-22 17:42:26.236989: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/leuven/icelake/2021a/software/CUDA/11.7.1/nvvm/lib64:/apps/leuven/icelake/2021a/software/CUDA/11.7.1/extras/CUPTI/lib64:/apps/leuven/icelake/2021a/software/CUDA/11.7.1/lib64\n",
      "2023-04-22 17:42:26.237066: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/leuven/icelake/2021a/software/CUDA/11.7.1/nvvm/lib64:/apps/leuven/icelake/2021a/software/CUDA/11.7.1/extras/CUPTI/lib64:/apps/leuven/icelake/2021a/software/CUDA/11.7.1/lib64\n",
      "2023-04-22 17:42:26.237074: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# Load the models\n",
    "model = VisionEncoderDecoderModel.from_pretrained(MODEL_NAME)\n",
    "image_processor = ViTImageProcessor.from_pretrained(MODEL_NAME)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(MODEL_NAME)\n",
    "# Place the model on the correct device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7275be",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "Since the goal is to keep the Televic-provided images as testset, a publicly available data set is required to validate the performance of the model, and finetune it if necessary. The dataset used in this notebook is the RSTPReid (Real Scenario Textbased Person Re-identification) dataset.  \n",
    "This dataset consists of 20505 images of 4101 persons from 15 cameras. Each person has 5 corresponding images taken by different cameras with complex scene transformations and backgrounds. Each image is annotated with 2 textual descriptions. The dataset comes pre-split into train-, validation-, and testsets, according to a 90-5-5 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f16a8e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in the training set: 18505\n",
      "Number of elements in the validation set: 1000\n",
      "Number of elements in the test set: 1000\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "train_set = RSTPReid(DATASET_PATH, 'train', image_processor)\n",
    "val_set = RSTPReid(DATASET_PATH, 'val', image_processor)\n",
    "test_set = RSTPReid(DATASET_PATH, 'test', image_processor)\n",
    "\n",
    "print(f\"Number of elements in the training set: {len(train_set)}\")\n",
    "print(f\"Number of elements in the validation set: {len(val_set)}\")\n",
    "print(f\"Number of elements in the test set: {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7e991b",
   "metadata": {},
   "source": [
    "## Defining an evaluation metric\n",
    "To evaluate how well our model performs on the previously defined dataset, an evaluation metric is required. Commonly used metrics consist of CIDEr, METEOR, ROUGE, and BLUE. It is known that these metrics don't always correlate well with human judgement, and that each metric has well known blind spots. However, they are easy to use, and are often implemented in popular machine learning libraries. Since our use case is rather simplistic, they will suffice.  \n",
    "\n",
    "- CIDEr (Consensus-based Image Description Evaluation) is a simple metric, which was defined specifically for image classification. However, since it is very new, there are no verified implementations in common libraries.\n",
    "- METEOR (Metric for Evaluation of Translation with Explicit Ordering) is a metric for the evaluation of machine translation output. Since this is not our use case, this metric might be less suitable.\n",
    "- ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics used for evaluating automatic summarization and machine translation. Since our use case can (generously) be described as summarizing an image into a short sentence, this metric might be suitable.\n",
    "- BLUE (Bilingual evaluation understudy) is an algorithm for evaluating the quality of a text which has been machine translated from one natural language to another. Since the use case here is not machine translation, this metric might not be very well suited.\n",
    "It should be noted that a more naive approach would be to simple use the Word Error Rate. However, this is a bad scoring way, since it does not account for synonyms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0185086",
   "metadata": {},
   "source": [
    "## Retraining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78b8d118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're finetuning 200 of the 444 parameters.\n"
     ]
    }
   ],
   "source": [
    "# Freeze all parameters that are a part of the decoder (GTP-2)\n",
    "for param in model.decoder.parameters():\n",
    "     param.requires_grad = False\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "print(f\"We're finetuning {len(params)} of the {len(list(model.parameters()))} parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78cd7b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset to a dataloader\n",
    "dataloader = torch.utils.data.DataLoader(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf7c5f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an optimizer and a learning rate scheduler\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08cc17a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "writername = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "writer = tensorboard.SummaryWriter(writername)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45f209fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " === EPOCH 0 === \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " === EPOCH 1 === \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " === EPOCH 2 === \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " === EPOCH 3 === \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " === EPOCH 4 === \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    }
   ],
   "source": [
    "# Start finetuning\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\" === EPOCH {epoch} === \")\n",
    "    train_one_epoch(\n",
    "        model=model, \n",
    "        optimizer=optimizer, \n",
    "        tokenizer=tokenizer, \n",
    "        data_loader=dataloader, \n",
    "        epoch=epoch, \n",
    "        device=device,\n",
    "        writer=writer\n",
    "    )\n",
    "    model.save_pretrained(f\"retrained_temp_epoch_{epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87cd9aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"baseline_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b6d19b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvnlp",
   "language": "python",
   "name": "cvnlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
